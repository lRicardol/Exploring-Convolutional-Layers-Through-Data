{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d58aacf",
   "metadata": {},
   "source": [
    "# Exploring Convolutional Layers Through Data and Experiments\n",
    "## Fashion-MNIST Case Study\n",
    "\n",
    "### Context and Motivation\n",
    "In this project, neural networks are treated as architectural components rather than black boxes.\n",
    "The goal is to understand how convolutional layers introduce inductive bias that improves learning\n",
    "on image-based data.\n",
    "\n",
    "Using the Fashion-MNIST dataset, we compare a baseline fully connected network against a\n",
    "convolutional neural network (CNN), and perform controlled experiments to analyze the effect\n",
    "of convolutional design choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f92ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e719861e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: No se puede encontrar el módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Santi\\Documents\\Libros de la universidad\\noveno\\trabajos de TDSE\\Exploring-Convolutional-Layers-Through-Data\\.venv\\Lib\\site-packages\\torch\\__init__.py:431\u001b[39m\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[32m    430\u001b[39m         _load_global_deps()\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSymInt\u001b[39;00m:\n\u001b[32m    435\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m \u001b[33;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[32m    437\u001b[39m \u001b[33;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[32m    438\u001b[39m \u001b[33;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[32m    439\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _C: No se puede encontrar el módulo especificado."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dcd5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed Fashion-MNIST CSV files\n",
    "train_df = pd.read_csv(\"data/processed/fashion-mnist-train.csv\")\n",
    "test_df  = pd.read_csv(\"data/processed/fashion-mnist-test.csv\")\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set shape:\", train_df.shape)\n",
    "print(\"Test set shape:\", test_df.shape)\n",
    "\n",
    "train_df[\"label\"].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cfba2a",
   "metadata": {},
   "source": [
    "- Each sample contains 784 numerical pixel values (28×28 image)\n",
    "- Labels range from 0 to 9 (10 clothing categories)\n",
    "- Images are grayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7157dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(df, n=6):\n",
    "    plt.figure(figsize=(8,3))\n",
    "    for i in range(n):\n",
    "        pixels = df.iloc[i, 1:].values.reshape(28, 28)\n",
    "        label = df.iloc[i, 0]\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(pixels, cmap=\"gray\")\n",
    "        plt.title(f\"Label: {label}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_samples(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5c48c",
   "metadata": {},
   "source": [
    "Preprocessing steps:\n",
    "- Normalize pixel values to [0, 1]\n",
    "- Reshape data for CNN input\n",
    "- Convert labels to tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a4b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.iloc[:, 1:].values / 255.0\n",
    "y_train = train_df.iloc[:, 0].values\n",
    "\n",
    "X_test = test_df.iloc[:, 1:].values / 255.0\n",
    "y_test = test_df.iloc[:, 0].values\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890cc53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx].reshape(1, 28, 28)  # 1 channel\n",
    "        label = self.y[idx]\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6db501",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FashionMNISTDataset(X_train, y_train)\n",
    "test_dataset  = FashionMNISTDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f0abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = BaselineNN()\n",
    "sum(p.numel() for p in baseline_model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04678c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        history.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bf78c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_history = train_model(baseline_model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70503893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = FashionMNISTCNN()\n",
    "sum(p.numel() for p in cnn_model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_history = train_model(cnn_model, train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa134e5",
   "metadata": {},
   "source": [
    "Experiment:\n",
    "- Compare kernel size 3×3 vs 5×5\n",
    "- Keep all other parameters fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24aaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNKernel5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 14 * 14, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(self.conv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87465454",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_k5 = CNNKernel5()\n",
    "train_model(cnn_k5, train_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e383962c",
   "metadata": {},
   "source": [
    "### Why did convolutional layers outperform the baseline?\n",
    "Convolutional layers exploit spatial locality and weight sharing, reducing the number of parameters\n",
    "while preserving spatial structure.\n",
    "\n",
    "### What inductive bias does convolution introduce?\n",
    "Translation invariance and local feature extraction.\n",
    "\n",
    "### When is convolution not appropriate?\n",
    "For non-spatial data such as tabular business metrics or symbolic data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47998bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained CNN model\n",
    "torch.save(cnn_model.state_dict(), \"model.pth\")\n",
    "print(\"Model saved as model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f31dfb",
   "metadata": {},
   "source": [
    "This experiment demonstrates that convolutional layers are not merely performance optimizations,\n",
    "but architectural components that encode domain assumptions.\n",
    "\n",
    "Understanding these assumptions is critical for designing robust and explainable AI systems\n",
    "in enterprise environments.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
